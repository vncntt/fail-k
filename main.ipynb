{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail@k Evaluation Framework\n",
    "\n",
    "This notebook implements a framework for evaluating language models using both standard accuracy metrics and the fail@k metric.\n",
    "\n",
    "## What is fail@k?\n",
    "\n",
    "Unlike pass@k (which counts a question as correct if any of k attempts succeeds), fail@k only counts a question as correct if ALL k attempts are correct. This is a much stricter test of model reliability.\n",
    "\n",
    "- pass@k tests the upper bound of model potential\n",
    "- fail@k tests the lower bound of model reliability\n",
    "\n",
    "As noted in the planning document, if a model is 95% reliable, you still have to check every output. But if it's 99.5% reliable, you might fully trust the model without manual verification.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Organized directory structure**: All results are stored in a structured way by dataset and evaluation type\n",
    "- **Standardized metrics**: Both regular accuracy and fail@k are implemented consistently\n",
    "- **Metadata tracking**: Each run is tracked with timestamps and detailed results\n",
    "- **Visualization tools**: Built-in functions to visualize and compare results\n",
    "- **Async option**: Both synchronous and asynchronous evaluation modes\n",
    "- **Multi-model comparison**: Easily compare multiple models on the same dataset\n",
    "- **Full experiment automation**: Run comprehensive experiments with a single function call\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To run an experiment, first ensure your environment variables are set up correctly (particularly `OPENROUTER_API_KEY`), then use:\n",
    "1. The individual test functions (`run_accuracy_test`, `run_fail_at_k_test`) for specific experiments\n",
    "2. Visualization functions (`visualize_results`, `compare_models`) to analyze results\n",
    "3. The `run_full_experiment` function to run a complete battery of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create output directories structure\n",
    "def create_output_dirs():\n",
    "    \"\"\"Create organized directory structure for experiment outputs\"\"\"\n",
    "    # Base output directory\n",
    "    base_dir = Path(\"results\")\n",
    "    base_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Dataset directories\n",
    "    datasets = [\"gsm8k\", \"math500\", \"gpqa\"]\n",
    "    for dataset in datasets:\n",
    "        # Create dataset directory\n",
    "        dataset_dir = base_dir / dataset\n",
    "        dataset_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories for different evaluation types\n",
    "        (dataset_dir / \"regular\").mkdir(exist_ok=True)\n",
    "        (dataset_dir / \"fail_at_k\").mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create metadata dir for summary results\n",
    "        (dataset_dir / \"metadata\").mkdir(exist_ok=True)\n",
    "\n",
    "# Create directories\n",
    "create_output_dirs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of gsm8k:  500\n"
     ]
    }
   ],
   "source": [
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "gsm8k_answers = []\n",
    "for i in range(len(gsm8k[\"train\"])):\n",
    "    sample = gsm8k[\"train\"][i]\n",
    "    for line in sample['answer'].split(\"\\n\"):\n",
    "        if line.strip().startswith(\"####\"):\n",
    "            gsm8k_answers.append(int(line.replace(\"####\", \"\").strip().replace(\",\",\"\")))\n",
    "\n",
    "gsm8k = gsm8k[\"train\"]['question']\n",
    "\n",
    "gsm8k = gsm8k[:500]\n",
    "gsm8k_answers = gsm8k_answers[:500]\n",
    "\n",
    "print(\"length of gsm8k: \", len(gsm8k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of math500:  500\n"
     ]
    }
   ],
   "source": [
    "math500 = load_dataset(\"HuggingFaceH4/MATH-500\")\n",
    "math500_answers = math500['test']['answer']\n",
    "math500 = math500['test']['problem']\n",
    "print(\"length of math500: \", len(math500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of gpqa:  448\n"
     ]
    }
   ],
   "source": [
    "gpqa = load_dataset(\"Idavidrein/gpqa\",'gpqa_main')\n",
    "gpqa_answers = gpqa['train']['Correct Answer']\n",
    "gpqa = gpqa['train']['Question']\n",
    "print(\"length of gpqa: \", len(gpqa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcb_codegen = load_dataset(\"livecodebench/code_generation_lite\", version_tag=\"release_v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(\n",
    "    prompt: str, \n",
    "    answer: str, \n",
    "    dataset: str, \n",
    "    model: str,\n",
    "    run_id: str = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Makes a request to the model API to get an answer for a math problem.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The math problem to solve\n",
    "        answer: The correct answer (for logging purposes)\n",
    "        dataset: Name of the dataset (gsm8k, math500, etc.)\n",
    "        model: Model identifier (e.g., \"anthropic/claude-3-5-sonnet\")\n",
    "        run_id: Optional identifier for the specific experiment run\n",
    "    \n",
    "    Returns:\n",
    "        The model's extracted answer as a string\n",
    "    \"\"\"\n",
    "    # Create paths based on the dataset and run info\n",
    "    base_dir = Path(\"results\") / dataset\n",
    "    run_id = run_id or f\"{int(time.time())}\"\n",
    "    \n",
    "    # Create output file paths\n",
    "    output_dir = base_dir / \"regular\"\n",
    "    output_file = output_dir / f\"{model.split('/')[-1]}_{run_id}.txt\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {os.getenv('OPENROUTER_API_KEY')}\",\n",
    "            },\n",
    "            data=json.dumps({\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": (\n",
    "                            \"\"\"\n",
    "                            Solve the math problem provided below. \n",
    "                            At the very end of your message, provide the answer to the problem in <ANSWER> </ANSWER> tags:\n",
    "                            In the answer tags, ONLY provide the answer to the problem. \n",
    "                            No other text or symbols such as $.\n",
    "                            Provide your answer in the simpliest form. Examples: 5 instead of 5.00 and 1/2 instead of 2/4.\n",
    "                            \"\"\"\n",
    "                            + prompt\n",
    "                        )\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            model_response = data['choices'][0]['message']['content']\n",
    "            \n",
    "            try:\n",
    "                model_answer = model_response.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip()\n",
    "            except IndexError:\n",
    "                model_answer = \"MISSING_ANSWER_TAGS\"\n",
    "            \n",
    "            # Ensure the output directory exists\n",
    "            output_dir.mkdir(exist_ok=True, parents=True)\n",
    "            \n",
    "            # Write the results to the output file\n",
    "            with open(output_file, 'a') as f:\n",
    "                f.write(\"QUESTION: \" + prompt + \"\\n\")\n",
    "                f.write(\"MODEL RESPONSE: \" + model_response + \"\\n\\n\")\n",
    "                f.write(\"MODEL ANSWER: \" + model_answer + \"\\n\\n\")\n",
    "                f.write(\"CORRECT ANSWER: \" + answer + \"\\n\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "            return model_answer\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            return \"ERROR_RESPONSE\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"ERROR_RESPONSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_answer(\n",
    "    model_answer: str, \n",
    "    correct_answer: str, \n",
    "    dataset: str,\n",
    "    model: str = \"anthropic/claude-3-5-sonnet\",\n",
    "    run_id: str = None\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the model's answer is equivalent to the correct answer using another LLM.\n",
    "    \n",
    "    Args:\n",
    "        model_answer: The answer provided by the model\n",
    "        correct_answer: The correct answer to compare against\n",
    "        dataset: Name of the dataset (gsm8k, math500, etc.)\n",
    "        model: Model identifier (e.g., \"anthropic/claude-3-5-sonnet\")\n",
    "        run_id: Optional identifier for the specific experiment run\n",
    "    \n",
    "    Returns:\n",
    "        Boolean indicating whether the answers are equivalent\n",
    "    \"\"\"\n",
    "    # Create paths based on the dataset and run info\n",
    "    base_dir = Path(\"results\") / dataset\n",
    "    run_id = run_id or f\"{int(time.time())}\"\n",
    "    \n",
    "    # Create output file paths\n",
    "    output_dir = base_dir / \"regular\"\n",
    "    check_file = output_dir / f\"checking_{model.split('/')[-1]}_{run_id}.txt\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {os.getenv('OPENROUTER_API_KEY')}\",\n",
    "            },\n",
    "            data=json.dumps({\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": (\n",
    "                            \"\"\"\n",
    "                            Check whether or not these two answers are equivalent.\n",
    "                            At the very end of your message, provide the answer to the problem in <ANSWER> </ANSWER> tags:\n",
    "                            If the answers are equivalent, there should only be \"YES\" in the answer tags.\n",
    "                            If they aren't equivalent, there should only be \"NO\" in the answer tags.\n",
    "                            Just a quick check is fine.\n",
    "                            No need for elaborate reasoning.\n",
    "                            First answer: \"\"\" + model_answer + \"\\n\" + \"Second answer: \" + correct_answer\n",
    "                        )\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            model_response = data['choices'][0]['message']['content'] # raw response\n",
    "            \n",
    "            try:\n",
    "                responses_equivalent = model_response.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip() # YES or NO\n",
    "            except IndexError:\n",
    "                responses_equivalent = \"NO\"\n",
    "                \n",
    "            final_answer = responses_equivalent == \"YES\" # true or false based \n",
    "            \n",
    "            # Ensure the output directory exists\n",
    "            output_dir.mkdir(exist_ok=True, parents=True)\n",
    "            \n",
    "            # Write the check results\n",
    "            with open(check_file, 'a') as f:\n",
    "                f.write(\"CORRECT ANSWER: \" + correct_answer + '\\n')\n",
    "                f.write(\"MODEL OUTPUT: \" + model_answer + '\\n')\n",
    "                f.write(\"COMPARISON ANSWER: \")\n",
    "                f.write(\"TRUE\\n\" if final_answer else \"FALSE\\n\")\n",
    "                f.write(model_response + '\\n')\n",
    "                f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "            return final_answer\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_accuracy_test(\n",
    "    questions: list[str], \n",
    "    answers: list[str], \n",
    "    dataset: str, \n",
    "    model: str,\n",
    "    max_samples: int = None,\n",
    "    return_details: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Runs an accuracy test on a dataset with a specific model.\n",
    "    \n",
    "    Args:\n",
    "        questions: List of questions to evaluate\n",
    "        answers: List of correct answers\n",
    "        dataset: Name of the dataset (gsm8k, math500, etc.)\n",
    "        model: Model identifier (e.g., \"anthropic/claude-3-5-sonnet\")\n",
    "        max_samples: Maximum number of samples to evaluate (useful for testing)\n",
    "        return_details: Whether to return detailed results or just the accuracy score\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the test results, including:\n",
    "        - accuracy: The overall accuracy score\n",
    "        - run_id: The unique identifier for this test run\n",
    "        - model: The model used\n",
    "        - dataset: The dataset used\n",
    "        - timestamp: When the test was run\n",
    "        - results: List of individual question results (if return_details=True)\n",
    "    \"\"\"\n",
    "    # Create a unique run ID\n",
    "    run_id = f\"{int(time.time())}\"\n",
    "    \n",
    "    # Limit the number of samples if specified\n",
    "    if max_samples is not None:\n",
    "        questions = questions[:max_samples]\n",
    "        answers = answers[:max_samples]\n",
    "    \n",
    "    # Results to track\n",
    "    num_correct = 0\n",
    "    detailed_results = []\n",
    "    wrong_indices = []\n",
    "    \n",
    "    print(f\"RUNNING ACCURACY TEST: {model} on {dataset}\")\n",
    "    print(f\"Number of questions: {len(questions)}\")\n",
    "    \n",
    "    # Process each question\n",
    "    for i in range(len(questions)):\n",
    "        print(f\"Processing question {i+1}/{len(questions)}...\", end='\\r')\n",
    "        \n",
    "        # Get the model's answer\n",
    "        model_answer = get_model_response(\n",
    "            questions[i], \n",
    "            str(answers[i]), \n",
    "            dataset, \n",
    "            model, \n",
    "            run_id\n",
    "        )\n",
    "        \n",
    "        # Check if the answer is correct\n",
    "        correct = checking_answer(\n",
    "            model_answer, \n",
    "            str(answers[i]), \n",
    "            dataset, \n",
    "            model=model, \n",
    "            run_id=run_id\n",
    "        )\n",
    "        \n",
    "        # Track the results\n",
    "        if correct:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            wrong_indices.append(i)\n",
    "            print(f\"WRONG {i}\")\n",
    "        \n",
    "        # Store detailed result\n",
    "        if return_details:\n",
    "            detailed_results.append({\n",
    "                \"index\": i,\n",
    "                \"question\": questions[i],\n",
    "                \"correct_answer\": str(answers[i]),\n",
    "                \"model_answer\": model_answer,\n",
    "                \"is_correct\": correct\n",
    "            })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = num_correct / len(questions)\n",
    "    \n",
    "    # Create a metadata file to store the summary results\n",
    "    metadata_dir = Path(\"results\") / dataset / \"metadata\"\n",
    "    metadata_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Create a summary of the test run\n",
    "    summary = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"run_id\": run_id,\n",
    "        \"model\": model,\n",
    "        \"dataset\": dataset,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"num_questions\": len(questions),\n",
    "        \"num_correct\": num_correct,\n",
    "        \"wrong_indices\": wrong_indices\n",
    "    }\n",
    "    \n",
    "    # Add detailed results if requested\n",
    "    if return_details:\n",
    "        summary[\"results\"] = detailed_results\n",
    "    \n",
    "    # Save the summary as JSON\n",
    "    with open(metadata_dir / f\"accuracy_{model.split('/')[-1]}_{run_id}.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"ACCURACY for {model} on {dataset}: {accuracy:.4f}\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fail_at_k_test(\n",
    "    questions: list[str], \n",
    "    answers: list[str], \n",
    "    dataset: str, \n",
    "    model: str,\n",
    "    k: int = 4,\n",
    "    max_samples: int = None,\n",
    "    return_details: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Runs a fail@k test on a dataset with a specific model.\n",
    "    In fail@k, a question is only considered correct if the model gets it right\n",
    "    on ALL k attempts.\n",
    "    \n",
    "    Args:\n",
    "        questions: List of questions to evaluate\n",
    "        answers: List of correct answers\n",
    "        dataset: Name of the dataset (gsm8k, math500, etc.)\n",
    "        model: Model identifier (e.g., \"anthropic/claude-3-5-sonnet\")\n",
    "        k: Number of attempts per question\n",
    "        max_samples: Maximum number of samples to evaluate (useful for testing)\n",
    "        return_details: Whether to return detailed results or just the accuracy score\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the test results, including:\n",
    "        - fail_at_k: The overall fail@k score\n",
    "        - run_id: The unique identifier for this test run\n",
    "        - model: The model used\n",
    "        - dataset: The dataset used\n",
    "        - k: The number of attempts per question\n",
    "        - timestamp: When the test was run\n",
    "        - results: List of individual question results (if return_details=True)\n",
    "    \"\"\"\n",
    "    # Create a unique run ID\n",
    "    run_id = f\"fail{k}_{int(time.time())}\"\n",
    "    \n",
    "    # Limit the number of samples if specified\n",
    "    if max_samples is not None:\n",
    "        questions = questions[:max_samples]\n",
    "        answers = answers[:max_samples]\n",
    "    \n",
    "    # Results to track\n",
    "    num_passed = 0\n",
    "    detailed_results = []\n",
    "    \n",
    "    print(f\"RUNNING FAIL@{k} TEST: {model} on {dataset}\")\n",
    "    print(f\"Number of questions: {len(questions)}\")\n",
    "    \n",
    "    # Create output directories\n",
    "    base_dir = Path(\"results\") / dataset\n",
    "    output_dir = base_dir / \"fail_at_k\"\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Process each question\n",
    "    for i in range(len(questions)):\n",
    "        print(f\"Running question {i+1}/{len(questions)}\")\n",
    "        \n",
    "        # Initialize results for this question\n",
    "        question_passed = True\n",
    "        question_attempts = []\n",
    "        \n",
    "        # Make k attempts for this question\n",
    "        for j in range(k):\n",
    "            print(f\"  Attempt {j+1}/{k}...\", end='\\r')\n",
    "            \n",
    "            # Create file names for this specific attempt\n",
    "            attempt_output_file = output_dir / f\"{model.split('/')[-1]}_{run_id}_q{i}_a{j}.txt\"\n",
    "            attempt_check_file = output_dir / f\"checking_{model.split('/')[-1]}_{run_id}_q{i}_a{j}.txt\"\n",
    "            \n",
    "            # Get the model's answer\n",
    "            model_answer = get_model_response(\n",
    "                questions[i], \n",
    "                str(answers[i]), \n",
    "                dataset, \n",
    "                model, \n",
    "                f\"{run_id}_q{i}_a{j}\"\n",
    "            )\n",
    "            \n",
    "            # Check if the answer is correct\n",
    "            correct = checking_answer(\n",
    "                model_answer, \n",
    "                str(answers[i]), \n",
    "                dataset, \n",
    "                model=model, \n",
    "                run_id=f\"{run_id}_q{i}_a{j}\"\n",
    "            )\n",
    "            \n",
    "            # Track the result of this attempt\n",
    "            question_attempts.append({\n",
    "                \"attempt\": j,\n",
    "                \"model_answer\": model_answer,\n",
    "                \"is_correct\": correct\n",
    "            })\n",
    "            \n",
    "            # In fail@k, if any attempt fails, the whole question fails\n",
    "            if not correct:\n",
    "                question_passed = False\n",
    "                print(f\"  Failed at attempt {j+1}\")\n",
    "                break\n",
    "        \n",
    "        # If all attempts passed, increment the counter\n",
    "        if question_passed:\n",
    "            num_passed += 1\n",
    "            print(f\"  Question {i+1} PASSED (all {k} attempts correct)\")\n",
    "        \n",
    "        # Store detailed result for this question\n",
    "        if return_details:\n",
    "            detailed_results.append({\n",
    "                \"index\": i,\n",
    "                \"question\": questions[i],\n",
    "                \"correct_answer\": str(answers[i]),\n",
    "                \"passed\": question_passed,\n",
    "                \"attempts\": question_attempts\n",
    "            })\n",
    "    \n",
    "    # Calculate fail@k score\n",
    "    fail_at_k_score = num_passed / len(questions)\n",
    "    \n",
    "    # Create a metadata file to store the summary results\n",
    "    metadata_dir = Path(\"results\") / dataset / \"metadata\"\n",
    "    metadata_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Create a summary of the test run\n",
    "    summary = {\n",
    "        \"fail_at_k_score\": fail_at_k_score,\n",
    "        \"run_id\": run_id,\n",
    "        \"model\": model,\n",
    "        \"dataset\": dataset,\n",
    "        \"k\": k,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"num_questions\": len(questions),\n",
    "        \"num_passed\": num_passed\n",
    "    }\n",
    "    \n",
    "    # Add detailed results if requested\n",
    "    if return_details:\n",
    "        summary[\"results\"] = detailed_results\n",
    "    \n",
    "    # Save the summary as JSON\n",
    "    with open(metadata_dir / f\"fail{k}_{model.split('/')[-1]}_{run_id}.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"FAIL@{k} score for {model} on {dataset}: {fail_at_k_score:.4f}\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(dataset: str = None, run_id: str = None):\n",
    "    \"\"\"\n",
    "    Visualizes results from accuracy and fail@k tests.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Name of the dataset to visualize (or None for all datasets)\n",
    "        run_id: Specific run ID to visualize (or None for all runs)\n",
    "    \"\"\"\n",
    "    # Initialize data for visualization\n",
    "    accuracy_data = []\n",
    "    fail_at_k_data = []\n",
    "    \n",
    "    # Get all metadata directories\n",
    "    base_dir = Path(\"results\")\n",
    "    if dataset:\n",
    "        datasets = [dataset]\n",
    "    else:\n",
    "        datasets = [d.name for d in base_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    for ds in datasets:\n",
    "        metadata_dir = base_dir / ds / \"metadata\"\n",
    "        if not metadata_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        # Process all JSON files in the metadata directory\n",
    "        for json_file in metadata_dir.glob(\"*.json\"):\n",
    "            # Skip if a specific run_id was requested and this isn't it\n",
    "            if run_id and run_id not in json_file.name:\n",
    "                continue\n",
    "                \n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            # Add to appropriate dataset based on the file name\n",
    "            if \"accuracy\" in json_file.name:\n",
    "                accuracy_data.append({\n",
    "                    \"dataset\": data[\"dataset\"],\n",
    "                    \"model\": data[\"model\"],\n",
    "                    \"accuracy\": data[\"accuracy\"],\n",
    "                    \"timestamp\": data[\"timestamp\"],\n",
    "                    \"run_id\": data[\"run_id\"]\n",
    "                })\n",
    "            elif \"fail\" in json_file.name:\n",
    "                fail_at_k_data.append({\n",
    "                    \"dataset\": data[\"dataset\"],\n",
    "                    \"model\": data[\"model\"],\n",
    "                    \"fail_at_k_score\": data[\"fail_at_k_score\"],\n",
    "                    \"k\": data[\"k\"],\n",
    "                    \"timestamp\": data[\"timestamp\"],\n",
    "                    \"run_id\": data[\"run_id\"]\n",
    "                })\n",
    "    \n",
    "    # Create DataFrames for visualization\n",
    "    if accuracy_data:\n",
    "        accuracy_df = pd.DataFrame(accuracy_data)\n",
    "        print(\"== ACCURACY RESULTS ==\")\n",
    "        print(accuracy_df[[\"dataset\", \"model\", \"accuracy\", \"timestamp\"]])\n",
    "        \n",
    "        # Plot accuracy results\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for dataset in accuracy_df[\"dataset\"].unique():\n",
    "            dataset_df = accuracy_df[accuracy_df[\"dataset\"] == dataset]\n",
    "            for model in dataset_df[\"model\"].unique():\n",
    "                model_df = dataset_df[dataset_df[\"model\"] == model]\n",
    "                plt.bar(f\"{dataset} - {model.split('/')[-1]}\", model_df[\"accuracy\"].values[0])\n",
    "        \n",
    "        plt.title(\"Accuracy Results by Dataset and Model\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if fail_at_k_data:\n",
    "        fail_at_k_df = pd.DataFrame(fail_at_k_data)\n",
    "        print(\"\\n== FAIL@K RESULTS ==\")\n",
    "        print(fail_at_k_df[[\"dataset\", \"model\", \"k\", \"fail_at_k_score\", \"timestamp\"]])\n",
    "        \n",
    "        # Plot fail@k results\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for dataset in fail_at_k_df[\"dataset\"].unique():\n",
    "            dataset_df = fail_at_k_df[fail_at_k_df[\"dataset\"] == dataset]\n",
    "            for model in dataset_df[\"model\"].unique():\n",
    "                model_df = dataset_df[dataset_df[\"model\"] == model]\n",
    "                for k in model_df[\"k\"].unique():\n",
    "                    k_df = model_df[model_df[\"k\"] == k]\n",
    "                    plt.bar(f\"{dataset} - {model.split('/')[-1]} (k={k})\", k_df[\"fail_at_k_score\"].values[0])\n",
    "        \n",
    "        plt.title(\"Fail@k Results by Dataset, Model, and k\")\n",
    "        plt.ylabel(\"Fail@k Score\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if not accuracy_data and not fail_at_k_data:\n",
    "        print(\"No results found for the specified criteria.\")\n",
    "\n",
    "def compare_models(dataset: str, models: list, max_samples: int = 20):\n",
    "    \"\"\"\n",
    "    Runs an accuracy test for multiple models on the same dataset and visualizes the results.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Name of the dataset to test on\n",
    "        models: List of model identifiers to compare\n",
    "        max_samples: Maximum number of samples to evaluate\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    if dataset == \"gsm8k\":\n",
    "        questions = gsm8k[:max_samples]\n",
    "        answers = gsm8k_answers[:max_samples]\n",
    "    elif dataset == \"math500\":\n",
    "        questions = math500[:max_samples]\n",
    "        answers = math500_answers[:max_samples]\n",
    "    elif dataset == \"gpqa\":\n",
    "        questions = gpqa[:max_samples]\n",
    "        answers = gpqa_answers[:max_samples]\n",
    "    else:\n",
    "        print(f\"Unknown dataset: {dataset}\")\n",
    "        return\n",
    "    \n",
    "    # Create a common run ID for this comparison\n",
    "    run_id = f\"compare_{int(time.time())}\"\n",
    "    \n",
    "    # Run tests for each model\n",
    "    results = []\n",
    "    for model in models:\n",
    "        print(f\"\\nTesting model: {model}\")\n",
    "        result = run_accuracy_test(\n",
    "            questions, \n",
    "            answers, \n",
    "            dataset, \n",
    "            model, \n",
    "            max_samples=max_samples,\n",
    "            return_details=False\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    # Create a DataFrame for comparison\n",
    "    comparison_df = pd.DataFrame([\n",
    "        {\n",
    "            \"model\": r[\"model\"].split('/')[-1],\n",
    "            \"accuracy\": r[\"accuracy\"],\n",
    "            \"num_correct\": r[\"num_correct\"],\n",
    "            \"num_questions\": r[\"num_questions\"]\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    # Print the comparison table\n",
    "    print(\"\\n== MODEL COMPARISON ==\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Visualize the comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(comparison_df[\"model\"], comparison_df[\"accuracy\"])\n",
    "    plt.title(f\"Model Accuracy Comparison on {dataset}\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ACCURACY TEST: anthropic/claude-3.5-sonnet on gsm8k\n",
      "Number of questions: 10\n",
      "ACCURACY for anthropic/claude-3.5-sonnet on gsm8k: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Example: Running a regular accuracy test\n",
    "gsm8k_test_result = run_accuracy_test(\n",
    "    gsm8k,  # Questions\n",
    "    gsm8k_answers,  # Answers\n",
    "    \"gsm8k\",  # Dataset name\n",
    "    \"anthropic/claude-3.5-sonnet\",  # Model to use\n",
    "    max_samples=10  # Limit to 10 samples for a quick test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING FAIL@2 TEST: anthropic/claude-3.5-sonnet on gsm8k\n",
      "Number of questions: 5\n",
      "Running question 1/5\n",
      "  Question 1 PASSED (all 2 attempts correct)\n",
      "Running question 2/5\n",
      "  Question 2 PASSED (all 2 attempts correct)\n",
      "Running question 3/5\n",
      "  Question 3 PASSED (all 2 attempts correct)\n",
      "Running question 4/5\n",
      "  Question 4 PASSED (all 2 attempts correct)\n",
      "Running question 5/5\n",
      "  Question 5 PASSED (all 2 attempts correct)\n",
      "FAIL@2 score for anthropic/claude-3.5-sonnet on gsm8k: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Example: Running a fail@k test\n",
    "fail_at_k_result = run_fail_at_k_test(\n",
    "    gsm8k,  # Questions\n",
    "    gsm8k_answers,  # Answers\n",
    "    \"gsm8k\",  # Dataset name\n",
    "    \"anthropic/claude-3.5-sonnet\",  # Model to use\n",
    "    k=2,  # Number of attempts per question\n",
    "    max_samples=5  # Limit to 5 samples for a quick test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing results for a specific dataset\n",
    "visualize_results(dataset=\"gsm8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Comparing multiple models on a dataset\n",
    "comparison = compare_models(\n",
    "    dataset=\"gsm8k\",\n",
    "    models=[\n",
    "        \"anthropic/claude-3-5-sonnet\",\n",
    "        \"google/gemini-1.5-pro\",\n",
    "        \"openai/gpt-4o\"\n",
    "    ],\n",
    "    max_samples=5  # Using just 5 samples for a quick test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING FAIL@K ACCURACY TESTS:\n",
      "Running the 0th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  15\n",
      "MODEL ANSWER:  15\n",
      "1th try\n",
      "CORRECT ANSWER:  15\n",
      "MODEL ANSWER:  15\n",
      "2th try\n",
      "CORRECT ANSWER:  15\n",
      "MODEL ANSWER:  15\n",
      "3th try\n",
      "CORRECT ANSWER:  15\n",
      "MODEL ANSWER:  15\n",
      "Running the 1th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  36\n",
      "MODEL ANSWER:  36\n",
      "1th try\n",
      "CORRECT ANSWER:  36\n",
      "MODEL ANSWER:  36\n",
      "2th try\n",
      "CORRECT ANSWER:  36\n",
      "MODEL ANSWER:  36\n",
      "3th try\n",
      "CORRECT ANSWER:  36\n",
      "MODEL ANSWER:  36\n",
      "Running the 2th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "1th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "2th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "3th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "Running the 3th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  258\n",
      "MODEL ANSWER:  258\n",
      "1th try\n",
      "CORRECT ANSWER:  258\n",
      "MODEL ANSWER:  258\n",
      "2th try\n",
      "CORRECT ANSWER:  258\n",
      "MODEL ANSWER:  258\n",
      "3th try\n",
      "CORRECT ANSWER:  258\n",
      "MODEL ANSWER:  258\n",
      "Running the 4th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  96\n",
      "MODEL ANSWER:  96\n",
      "1th try\n",
      "CORRECT ANSWER:  96\n",
      "MODEL ANSWER:  96\n",
      "2th try\n",
      "CORRECT ANSWER:  96\n",
      "MODEL ANSWER:  96\n",
      "3th try\n",
      "CORRECT ANSWER:  96\n",
      "MODEL ANSWER:  96\n",
      "Running the 5th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  320\n",
      "MODEL ANSWER:  320\n",
      "1th try\n",
      "CORRECT ANSWER:  320\n",
      "MODEL ANSWER:  320\n",
      "2th try\n",
      "CORRECT ANSWER:  320\n",
      "MODEL ANSWER:  320\n",
      "3th try\n",
      "CORRECT ANSWER:  320\n",
      "MODEL ANSWER:  320\n",
      "Running the 6th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  50\n",
      "MODEL ANSWER:  50\n",
      "1th try\n",
      "CORRECT ANSWER:  50\n",
      "MODEL ANSWER:  50\n",
      "2th try\n",
      "CORRECT ANSWER:  50\n",
      "MODEL ANSWER:  50\n",
      "3th try\n",
      "CORRECT ANSWER:  50\n",
      "MODEL ANSWER:  50\n",
      "Running the 7th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  97\n",
      "MODEL ANSWER:  97\n",
      "1th try\n",
      "CORRECT ANSWER:  97\n",
      "MODEL ANSWER:  97\n",
      "2th try\n",
      "CORRECT ANSWER:  97\n",
      "MODEL ANSWER:  97\n",
      "3th try\n",
      "CORRECT ANSWER:  97\n",
      "MODEL ANSWER:  97\n",
      "Running the 8th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  41\n",
      "MODEL ANSWER:  574\n",
      "Failed at the 0th example\n",
      "Running the 9th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  20\n",
      "MODEL ANSWER:  20\n",
      "1th try\n",
      "CORRECT ANSWER:  20\n",
      "MODEL ANSWER:  20\n",
      "2th try\n",
      "CORRECT ANSWER:  20\n",
      "MODEL ANSWER:  20\n",
      "3th try\n",
      "CORRECT ANSWER:  20\n",
      "MODEL ANSWER:  20\n",
      "Running the 10th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  140\n",
      "MODEL ANSWER:  140\n",
      "1th try\n",
      "CORRECT ANSWER:  140\n",
      "MODEL ANSWER:  140\n",
      "2th try\n",
      "CORRECT ANSWER:  140\n",
      "MODEL ANSWER:  140\n",
      "3th try\n",
      "CORRECT ANSWER:  140\n",
      "MODEL ANSWER:  140\n",
      "Running the 11th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "1th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "2th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "3th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "Running the 12th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  920\n",
      "MODEL ANSWER:  920\n",
      "1th try\n",
      "CORRECT ANSWER:  920\n",
      "MODEL ANSWER:  920\n",
      "2th try\n",
      "CORRECT ANSWER:  920\n",
      "MODEL ANSWER:  920\n",
      "3th try\n",
      "CORRECT ANSWER:  920\n",
      "MODEL ANSWER:  920\n",
      "Running the 13th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  60\n",
      "MODEL ANSWER:  60\n",
      "1th try\n",
      "CORRECT ANSWER:  60\n",
      "MODEL ANSWER:  60\n",
      "2th try\n",
      "CORRECT ANSWER:  60\n",
      "MODEL ANSWER:  60\n",
      "3th try\n",
      "CORRECT ANSWER:  60\n",
      "MODEL ANSWER:  60\n",
      "Running the 14th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  15\n",
      "MODEL ANSWER:  15\n",
      "1th try\n",
      "CORRECT ANSWER:  15\n",
      "MODEL ANSWER:  15\n",
      "2th try\n",
      "CORRECT ANSWER:  15\n",
      "MODEL ANSWER:  15\n",
      "3th try\n",
      "CORRECT ANSWER:  15\n",
      "MODEL ANSWER:  15\n",
      "Running the 15th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  4000\n",
      "MODEL ANSWER:  4000\n",
      "1th try\n",
      "CORRECT ANSWER:  4000\n",
      "MODEL ANSWER:  4000\n",
      "2th try\n",
      "CORRECT ANSWER:  4000\n",
      "MODEL ANSWER:  4000\n",
      "3th try\n",
      "CORRECT ANSWER:  4000\n",
      "MODEL ANSWER:  4000\n",
      "Running the 16th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  21\n",
      "MODEL ANSWER:  21\n",
      "1th try\n",
      "CORRECT ANSWER:  21\n",
      "MODEL ANSWER:  21\n",
      "2th try\n",
      "CORRECT ANSWER:  21\n",
      "MODEL ANSWER:  21\n",
      "3th try\n",
      "CORRECT ANSWER:  21\n",
      "MODEL ANSWER:  21\n",
      "Running the 17th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  16\n",
      "MODEL ANSWER:  16\n",
      "1th try\n",
      "CORRECT ANSWER:  16\n",
      "MODEL ANSWER:  16\n",
      "2th try\n",
      "CORRECT ANSWER:  16\n",
      "MODEL ANSWER:  16\n",
      "3th try\n",
      "CORRECT ANSWER:  16\n",
      "MODEL ANSWER:  16\n",
      "Running the 18th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  13\n",
      "MODEL ANSWER:  13\n",
      "1th try\n",
      "CORRECT ANSWER:  13\n",
      "MODEL ANSWER:  13\n",
      "2th try\n",
      "CORRECT ANSWER:  13\n",
      "MODEL ANSWER:  11\n",
      "Failed at the 2th example\n",
      "Running the 19th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  140\n",
      "MODEL ANSWER:  140\n",
      "1th try\n",
      "CORRECT ANSWER:  140\n",
      "MODEL ANSWER:  140\n",
      "2th try\n",
      "CORRECT ANSWER:  140\n",
      "MODEL ANSWER:  140\n",
      "3th try\n",
      "CORRECT ANSWER:  140\n",
      "MODEL ANSWER:  140\n",
      "Running the 20th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  720\n",
      "MODEL ANSWER:  720\n",
      "1th try\n",
      "CORRECT ANSWER:  720\n",
      "MODEL ANSWER:  720\n",
      "2th try\n",
      "CORRECT ANSWER:  720\n",
      "MODEL ANSWER:  720\n",
      "3th try\n",
      "CORRECT ANSWER:  720\n",
      "MODEL ANSWER:  720\n",
      "Running the 21th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "1th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "2th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "3th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "Running the 22th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  48\n",
      "MODEL ANSWER:  48\n",
      "1th try\n",
      "CORRECT ANSWER:  48\n",
      "MODEL ANSWER:  48\n",
      "2th try\n",
      "CORRECT ANSWER:  48\n",
      "MODEL ANSWER:  48\n",
      "3th try\n",
      "CORRECT ANSWER:  48\n",
      "MODEL ANSWER:  48\n",
      "Running the 23th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "1th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "2th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "3th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "Running the 24th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  18\n",
      "MODEL ANSWER:  18\n",
      "1th try\n",
      "CORRECT ANSWER:  18\n",
      "MODEL ANSWER:  18\n",
      "2th try\n",
      "CORRECT ANSWER:  18\n",
      "MODEL ANSWER:  18\n",
      "3th try\n",
      "CORRECT ANSWER:  18\n",
      "MODEL ANSWER:  18\n",
      "Running the 25th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  50\n",
      "MODEL ANSWER:  50\n",
      "1th try\n",
      "CORRECT ANSWER:  50\n",
      "MODEL ANSWER:  50\n",
      "2th try\n",
      "CORRECT ANSWER:  50\n",
      "MODEL ANSWER:  50\n",
      "3th try\n",
      "CORRECT ANSWER:  50\n",
      "MODEL ANSWER:  50\n",
      "Running the 26th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "1th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "2th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "3th try\n",
      "CORRECT ANSWER:  25\n",
      "MODEL ANSWER:  25\n",
      "Running the 27th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  55\n",
      "MODEL ANSWER:  55\n",
      "1th try\n",
      "CORRECT ANSWER:  55\n",
      "MODEL ANSWER:  55\n",
      "2th try\n",
      "CORRECT ANSWER:  55\n",
      "MODEL ANSWER:  55\n",
      "3th try\n",
      "CORRECT ANSWER:  55\n",
      "MODEL ANSWER:  55\n",
      "Running the 28th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "1th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "2th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "3th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "Running the 29th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  10\n",
      "MODEL ANSWER:  10\n",
      "1th try\n",
      "CORRECT ANSWER:  10\n",
      "MODEL ANSWER:  8\n",
      "Failed at the 1th example\n",
      "Running the 30th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  55\n",
      "MODEL ANSWER:  55\n",
      "1th try\n",
      "CORRECT ANSWER:  55\n",
      "MODEL ANSWER:  55\n",
      "2th try\n",
      "CORRECT ANSWER:  55\n",
      "MODEL ANSWER:  55\n",
      "3th try\n",
      "CORRECT ANSWER:  55\n",
      "MODEL ANSWER:  55\n",
      "Running the 31th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  500\n",
      "MODEL ANSWER:  500\n",
      "1th try\n",
      "CORRECT ANSWER:  500\n",
      "MODEL ANSWER:  500\n",
      "2th try\n",
      "CORRECT ANSWER:  500\n",
      "MODEL ANSWER:  500\n",
      "3th try\n",
      "CORRECT ANSWER:  500\n",
      "MODEL ANSWER:  500\n",
      "Running the 32th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  110\n",
      "MODEL ANSWER:  110\n",
      "1th try\n",
      "CORRECT ANSWER:  110\n",
      "MODEL ANSWER:  110\n",
      "2th try\n",
      "CORRECT ANSWER:  110\n",
      "MODEL ANSWER:  110\n",
      "3th try\n",
      "CORRECT ANSWER:  110\n",
      "MODEL ANSWER:  110\n",
      "Running the 33th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "1th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "2th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "3th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "Running the 34th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  74\n",
      "MODEL ANSWER:  74\n",
      "1th try\n",
      "CORRECT ANSWER:  74\n",
      "MODEL ANSWER:  74\n",
      "2th try\n",
      "CORRECT ANSWER:  74\n",
      "MODEL ANSWER:  74\n",
      "3th try\n",
      "CORRECT ANSWER:  74\n",
      "MODEL ANSWER:  74\n",
      "Running the 35th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  2350\n",
      "MODEL ANSWER:  2350\n",
      "1th try\n",
      "CORRECT ANSWER:  2350\n",
      "MODEL ANSWER:  2350\n",
      "2th try\n",
      "CORRECT ANSWER:  2350\n",
      "MODEL ANSWER:  2350\n",
      "3th try\n",
      "CORRECT ANSWER:  2350\n",
      "MODEL ANSWER:  2350\n",
      "Running the 36th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  20\n",
      "MODEL ANSWER:  20\n",
      "1th try\n",
      "CORRECT ANSWER:  20\n",
      "MODEL ANSWER:  20\n",
      "2th try\n",
      "CORRECT ANSWER:  20\n",
      "MODEL ANSWER:  20\n",
      "3th try\n",
      "CORRECT ANSWER:  20\n",
      "MODEL ANSWER:  20\n",
      "Running the 37th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "1th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "2th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "3th try\n",
      "CORRECT ANSWER:  6\n",
      "MODEL ANSWER:  6\n",
      "Running the 38th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  8\n",
      "MODEL ANSWER:  8\n",
      "1th try\n",
      "CORRECT ANSWER:  8\n",
      "MODEL ANSWER:  8\n",
      "2th try\n",
      "CORRECT ANSWER:  8\n",
      "MODEL ANSWER:  8\n",
      "3th try\n",
      "CORRECT ANSWER:  8\n",
      "MODEL ANSWER:  8\n",
      "Running the 39th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  24\n",
      "MODEL ANSWER:  24\n",
      "1th try\n",
      "CORRECT ANSWER:  24\n",
      "MODEL ANSWER:  24\n",
      "2th try\n",
      "CORRECT ANSWER:  24\n",
      "MODEL ANSWER:  24\n",
      "3th try\n",
      "CORRECT ANSWER:  24\n",
      "MODEL ANSWER:  24\n",
      "Running the 40th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  148\n",
      "MODEL ANSWER:  148\n",
      "1th try\n",
      "CORRECT ANSWER:  148\n",
      "MODEL ANSWER:  148\n",
      "2th try\n",
      "CORRECT ANSWER:  148\n",
      "MODEL ANSWER:  148\n",
      "3th try\n",
      "CORRECT ANSWER:  148\n",
      "MODEL ANSWER:  148\n",
      "Running the 41th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  72\n",
      "MODEL ANSWER:  72\n",
      "1th try\n",
      "CORRECT ANSWER:  72\n",
      "MODEL ANSWER:  72\n",
      "2th try\n",
      "CORRECT ANSWER:  72\n",
      "MODEL ANSWER:  72\n",
      "3th try\n",
      "CORRECT ANSWER:  72\n",
      "MODEL ANSWER:  72\n",
      "Running the 42th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  30\n",
      "MODEL ANSWER:  30\n",
      "1th try\n",
      "CORRECT ANSWER:  30\n",
      "MODEL ANSWER:  30\n",
      "2th try\n",
      "CORRECT ANSWER:  30\n",
      "MODEL ANSWER:  30\n",
      "3th try\n",
      "CORRECT ANSWER:  30\n",
      "MODEL ANSWER:  30\n",
      "Running the 43th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  45\n",
      "MODEL ANSWER:  45\n",
      "1th try\n",
      "CORRECT ANSWER:  45\n",
      "MODEL ANSWER:  45\n",
      "2th try\n",
      "CORRECT ANSWER:  45\n",
      "MODEL ANSWER:  45\n",
      "3th try\n",
      "CORRECT ANSWER:  45\n",
      "MODEL ANSWER:  45\n",
      "Running the 44th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  8\n",
      "MODEL ANSWER:  8\n",
      "1th try\n",
      "CORRECT ANSWER:  8\n",
      "MODEL ANSWER:  8\n",
      "2th try\n",
      "CORRECT ANSWER:  8\n",
      "MODEL ANSWER:  8\n",
      "3th try\n",
      "CORRECT ANSWER:  8\n",
      "MODEL ANSWER:  8\n",
      "Running the 45th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  2\n",
      "MODEL ANSWER:  2\n",
      "1th try\n",
      "CORRECT ANSWER:  2\n",
      "MODEL ANSWER:  2\n",
      "2th try\n",
      "CORRECT ANSWER:  2\n",
      "MODEL ANSWER:  2\n",
      "3th try\n",
      "CORRECT ANSWER:  2\n",
      "MODEL ANSWER:  2\n",
      "Running the 46th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  136\n",
      "MODEL ANSWER:  136\n",
      "1th try\n",
      "CORRECT ANSWER:  136\n",
      "MODEL ANSWER:  136\n",
      "2th try\n",
      "CORRECT ANSWER:  136\n",
      "MODEL ANSWER:  136\n",
      "3th try\n",
      "CORRECT ANSWER:  136\n",
      "MODEL ANSWER:  136\n",
      "Running the 47th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "1th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "2th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "3th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "Running the 48th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  35\n",
      "MODEL ANSWER:  35\n",
      "1th try\n",
      "CORRECT ANSWER:  35\n",
      "MODEL ANSWER:  35\n",
      "2th try\n",
      "CORRECT ANSWER:  35\n",
      "MODEL ANSWER:  35\n",
      "3th try\n",
      "CORRECT ANSWER:  35\n",
      "MODEL ANSWER:  35\n",
      "Running the 49th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  4\n",
      "MODEL ANSWER:  4\n",
      "1th try\n",
      "CORRECT ANSWER:  4\n",
      "MODEL ANSWER:  4\n",
      "2th try\n",
      "CORRECT ANSWER:  4\n",
      "MODEL ANSWER:  4\n",
      "3th try\n",
      "CORRECT ANSWER:  4\n",
      "MODEL ANSWER:  4\n",
      "ACCURACY FOR FAIL@K:  7.64\n"
     ]
    }
   ],
   "source": [
    "print(\"RUNNING FAIL@K ACCURACY TESTS:\")\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(gsm8k)):\n",
    "    passing = True\n",
    "    print(f\"Running the {i}th sample\")\n",
    "    for j in range(4):\n",
    "        print(f\"{j}th try\")\n",
    "        answer = get_model_response(gsm8k[i],str(gsm8k_answers[i]),\"fail.txt\")\n",
    "        correct = checking_answer(answer,str(gsm8k_answers[i]),\"checking_gsm8k_fail.txt\")\n",
    "        if correct:\n",
    "            correct+=1\n",
    "        else:\n",
    "            passing = False\n",
    "            print(f\"Failed at the {j}th example\")\n",
    "            break\n",
    "    if passing:\n",
    "        correct+=1\n",
    "\n",
    "print(\"ACCURACY FOR FAIL@K: \", correct/len(gsm8k_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for above is 0.94 (three failed examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INDICES OF PROBLEMS THE MODEL GETS WRONG:\")\n",
    "wrong_indices = [59,63,73, 108, 118, 167]\n",
    "print(wrong_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATH500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ACCURACY TESTS:\n",
      "CORRECT ANSWER:  \\left( 3, \\frac{\\pi}{2} \\right)\n",
      "MODEL ANSWER:  (3,/2)\n",
      "CORRECT\n",
      "CORRECT ANSWER:  p - q\n",
      "MODEL ANSWER:  p - q\n",
      "CORRECT\n",
      "CORRECT ANSWER:  \\frac{14}{3}\n",
      "MODEL ANSWER:  14/3\n",
      "CORRECT\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "CORRECT\n",
      "CORRECT ANSWER:  \\text{Evelyn}\n",
      "MODEL ANSWER:  Evelyn\n",
      "CORRECT\n",
      "CORRECT ANSWER:  42\n",
      "MODEL ANSWER:  42\n",
      "CORRECT\n",
      "CORRECT ANSWER:  27\n",
      "MODEL ANSWER:  27\n",
      "CORRECT\n",
      "CORRECT ANSWER:  90^\\circ\n",
      "MODEL ANSWER:  72\n",
      "WRONG\n",
      "CORRECT ANSWER:  3\\sqrt{13}\n",
      "MODEL ANSWER:  313\n",
      "CORRECT\n",
      "CORRECT ANSWER:  4\n",
      "MODEL ANSWER:  2\n",
      "WRONG\n",
      "CORRECT ANSWER:  2220\n",
      "MODEL ANSWER:  2220\n",
      "CORRECT\n",
      "CORRECT ANSWER:  \\frac{3}{56}\n",
      "MODEL ANSWER:  8/63\n",
      "WRONG\n",
      "CORRECT ANSWER:  284\n",
      "MODEL ANSWER:  284\n",
      "CORRECT\n",
      "CORRECT ANSWER:  5\n",
      "MODEL ANSWER:  5\n",
      "CORRECT\n",
      "CORRECT ANSWER:  \\sqrt{51}\n",
      "MODEL ANSWER:  51\n",
      "CORRECT\n",
      "CORRECT ANSWER:  6 - 5i\n",
      "MODEL ANSWER:  6 - 5i\n",
      "CORRECT\n",
      "CORRECT ANSWER:  -50\n",
      "MODEL ANSWER:  -50\n",
      "CORRECT\n",
      "CORRECT ANSWER:  \\pi\n",
      "MODEL ANSWER:  \n",
      "CORRECT\n",
      "CORRECT ANSWER:  28\n",
      "MODEL ANSWER:  56\n",
      "WRONG\n",
      "CORRECT ANSWER:  3\n",
      "MODEL ANSWER:  3+22\n",
      "WRONG\n",
      "CORRECT ANSWER:  6+9i\n",
      "MODEL ANSWER:  6 + 9i\n",
      "CORRECT\n",
      "CORRECT ANSWER:  13535\n",
      "MODEL ANSWER:  15624\n",
      "WRONG\n",
      "CORRECT ANSWER:  5\n",
      "MODEL ANSWER:  5\n",
      "CORRECT\n",
      "CORRECT ANSWER:  x=5\n",
      "MODEL ANSWER:  5\n",
      "CORRECT\n",
      "CORRECT ANSWER:  10\n",
      "MODEL ANSWER:  10\n",
      "CORRECT\n",
      "CORRECT ANSWER:  1,-2\n",
      "MODEL ANSWER:  1\n",
      "WRONG\n",
      "CORRECT ANSWER:  144\n",
      "MODEL ANSWER:  384\n",
      "WRONG\n",
      "CORRECT ANSWER:  78\n",
      "MODEL ANSWER:  78\n",
      "CORRECT\n",
      "CORRECT ANSWER:  -2 + 7i\n",
      "MODEL ANSWER:  -2 + 7i\n",
      "CORRECT\n",
      "CORRECT ANSWER:  225\n",
      "MODEL ANSWER:  225\n",
      "CORRECT\n",
      "ACCURACY for regular:  0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "length = 30\n",
    "math500_subset_questions = math500[:length]\n",
    "math500_subset_answers = math500_answers[:length]\n",
    "\n",
    "with open('math500.txt', 'w') as f:\n",
    "    f.write('')\n",
    "with open('checking_math500.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "\n",
    "print(\"RUNNING ACCURACY TESTS:\")\n",
    "\n",
    "correct_count = 0\n",
    "for i in range(length):\n",
    "    answer = get_model_response(math500_subset_questions[i],math500_subset_answers[i],'math500.txt')\n",
    "\n",
    "    correct = checking_answer(answer,math500_subset_answers[i],\"checking_math500.txt\")\n",
    "    if correct:\n",
    "        correct_count +=1\n",
    "        print(\"CORRECT\")\n",
    "    else:\n",
    "        print(\"WRONG\")\n",
    "\n",
    "\n",
    "print(\"ACCURACY for regular: \", correct_count/len(math500_subset_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING TESTS FOR FAIL@4:\n",
      "Running the 0th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  \\left( 3, \\frac{\\pi}{2} \\right)\n",
      "MODEL ANSWER:  (3, /2)\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  \\left( 3, \\frac{\\pi}{2} \\right)\n",
      "MODEL ANSWER:  (3,/2)\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  \\left( 3, \\frac{\\pi}{2} \\right)\n",
      "MODEL ANSWER:  (3,/2)\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  \\left( 3, \\frac{\\pi}{2} \\right)\n",
      "MODEL ANSWER:  (3,1.5708)\n",
      "correct\n",
      "model passed the 0th sample\n",
      "Running the 1th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  p - q\n",
      "MODEL ANSWER:  p - q\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  p - q\n",
      "MODEL ANSWER:  p - q\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  p - q\n",
      "MODEL ANSWER:  p-q\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  p - q\n",
      "MODEL ANSWER:  p-q\n",
      "correct\n",
      "model passed the 1th sample\n",
      "Running the 2th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  \\frac{14}{3}\n",
      "MODEL ANSWER:  14/3\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  \\frac{14}{3}\n",
      "MODEL ANSWER:  14/3\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  \\frac{14}{3}\n",
      "MODEL ANSWER:  14/3\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  \\frac{14}{3}\n",
      "MODEL ANSWER:  14/3\n",
      "correct\n",
      "model passed the 2th sample\n",
      "Running the 3th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  9\n",
      "MODEL ANSWER:  9\n",
      "correct\n",
      "model passed the 3th sample\n",
      "Running the 4th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  \\text{Evelyn}\n",
      "MODEL ANSWER:  Evelyn\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  \\text{Evelyn}\n",
      "MODEL ANSWER:  Evelyn\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  \\text{Evelyn}\n",
      "MODEL ANSWER:  Evelyn\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  \\text{Evelyn}\n",
      "MODEL ANSWER:  Evelyn\n",
      "correct\n",
      "model passed the 4th sample\n",
      "Running the 5th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  42\n",
      "MODEL ANSWER:  42\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  42\n",
      "MODEL ANSWER:  42\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  42\n",
      "MODEL ANSWER:  42\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  42\n",
      "MODEL ANSWER:  42\n",
      "correct\n",
      "model passed the 5th sample\n",
      "Running the 6th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  27\n",
      "MODEL ANSWER:  27\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  27\n",
      "MODEL ANSWER:  27\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  27\n",
      "MODEL ANSWER:  27\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  27\n",
      "MODEL ANSWER:  27\n",
      "correct\n",
      "model passed the 6th sample\n",
      "Running the 7th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  90^\\circ\n",
      "MODEL ANSWER:  87.6\n",
      "Failed at the 0th example\n",
      "Running the 8th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  3\\sqrt{13}\n",
      "MODEL ANSWER:  313\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  3\\sqrt{13}\n",
      "MODEL ANSWER:  313\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  3\\sqrt{13}\n",
      "MODEL ANSWER:  313\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  3\\sqrt{13}\n",
      "MODEL ANSWER:  313\n",
      "correct\n",
      "model passed the 8th sample\n",
      "Running the 9th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  4\n",
      "MODEL ANSWER:  2\n",
      "Failed at the 0th example\n",
      "Running the 10th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  2220\n",
      "MODEL ANSWER:  2000\n",
      "Failed at the 0th example\n",
      "Running the 11th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  \\frac{3}{56}\n",
      "MODEL ANSWER:  -2/63\n",
      "Failed at the 0th example\n",
      "Running the 12th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  284\n",
      "MODEL ANSWER:  284\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  284\n",
      "MODEL ANSWER:  284\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  284\n",
      "MODEL ANSWER:  284\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  284\n",
      "MODEL ANSWER:  284\n",
      "correct\n",
      "model passed the 12th sample\n",
      "Running the 13th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  5\n",
      "MODEL ANSWER:  5\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  5\n",
      "MODEL ANSWER:  5\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  5\n",
      "MODEL ANSWER:  5\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  5\n",
      "MODEL ANSWER:  5\n",
      "correct\n",
      "model passed the 13th sample\n",
      "Running the 14th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  \\sqrt{51}\n",
      "MODEL ANSWER:  51\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  \\sqrt{51}\n",
      "MODEL ANSWER:  $\\sqrt{51}$\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  \\sqrt{51}\n",
      "MODEL ANSWER:  51\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  \\sqrt{51}\n",
      "MODEL ANSWER:  10\n",
      "Failed at the 3th example\n",
      "Running the 15th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  6 - 5i\n",
      "MODEL ANSWER:  -5i\n",
      "Failed at the 0th example\n",
      "Running the 16th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  -50\n",
      "MODEL ANSWER:  -50\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  -50\n",
      "MODEL ANSWER:  -50\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  -50\n",
      "MODEL ANSWER:  -50\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  -50\n",
      "MODEL ANSWER:  -50\n",
      "correct\n",
      "model passed the 16th sample\n",
      "Running the 17th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  \\pi\n",
      "MODEL ANSWER:  /2\n",
      "Failed at the 0th example\n",
      "Running the 18th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  28\n",
      "MODEL ANSWER:  56\n",
      "Failed at the 0th example\n",
      "Running the 19th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  3\n",
      "MODEL ANSWER:  3\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  3\n",
      "MODEL ANSWER:  3\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  3\n",
      "MODEL ANSWER:  3\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  3\n",
      "MODEL ANSWER:  3\n",
      "correct\n",
      "model passed the 19th sample\n",
      "Running the 20th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  6+9i\n",
      "MODEL ANSWER:  6+9i\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  6+9i\n",
      "MODEL ANSWER:  6+9i\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  6+9i\n",
      "MODEL ANSWER:  6+9i\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  6+9i\n",
      "MODEL ANSWER:  6+9i\n",
      "correct\n",
      "model passed the 20th sample\n",
      "Running the 21th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  13535\n",
      "MODEL ANSWER:  46340\n",
      "Failed at the 0th example\n",
      "Running the 22th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  5\n",
      "MODEL ANSWER:  5\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  5\n",
      "MODEL ANSWER:  4\n",
      "Failed at the 1th example\n",
      "Running the 23th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  x=5\n",
      "MODEL ANSWER:  5\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  x=5\n",
      "MODEL ANSWER:  5\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  x=5\n",
      "MODEL ANSWER:  5\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  x=5\n",
      "MODEL ANSWER:  5\n",
      "correct\n",
      "model passed the 23th sample\n",
      "Running the 24th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  10\n",
      "MODEL ANSWER:  10\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  10\n",
      "MODEL ANSWER:  8\n",
      "Failed at the 1th example\n",
      "Running the 25th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  1,-2\n",
      "MODEL ANSWER:  -2,1\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  1,-2\n",
      "MODEL ANSWER:  1,2\n",
      "Failed at the 1th example\n",
      "Running the 26th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  144\n",
      "MODEL ANSWER:  384\n",
      "Failed at the 0th example\n",
      "Running the 27th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  78\n",
      "MODEL ANSWER:  78\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  78\n",
      "MODEL ANSWER:  78\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  78\n",
      "MODEL ANSWER:  78\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  78\n",
      "MODEL ANSWER:  78\n",
      "correct\n",
      "model passed the 27th sample\n",
      "Running the 28th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  -2 + 7i\n",
      "MODEL ANSWER:  -2 + 7i\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  -2 + 7i\n",
      "MODEL ANSWER:  -2 + 7i\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  -2 + 7i\n",
      "MODEL ANSWER:  -2 + 7i\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  -2 + 7i\n",
      "MODEL ANSWER:  -2 + 7i\n",
      "correct\n",
      "model passed the 28th sample\n",
      "Running the 29th sample\n",
      "0th try\n",
      "CORRECT ANSWER:  225\n",
      "MODEL ANSWER:  225\n",
      "correct\n",
      "1th try\n",
      "CORRECT ANSWER:  225\n",
      "MODEL ANSWER:  225\n",
      "correct\n",
      "2th try\n",
      "CORRECT ANSWER:  225\n",
      "MODEL ANSWER:  225\n",
      "correct\n",
      "3th try\n",
      "CORRECT ANSWER:  225\n",
      "MODEL ANSWER:  225\n",
      "correct\n",
      "model passed the 29th sample\n",
      "ACCURACY for FAIL@4:  0.0\n"
     ]
    }
   ],
   "source": [
    "length = 30\n",
    "math500_subset_questions = math500[:length]\n",
    "math500_subset_answers = math500_answers[:length]\n",
    "\n",
    "with open('math500_fail.txt', 'w') as f:\n",
    "    f.write('')\n",
    "with open('checking_math500_fail.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "\n",
    "print(\"RUNNING TESTS FOR FAIL@4:\")\n",
    "\n",
    "correct_count = 0\n",
    "for i in range(length):\n",
    "    passing = True\n",
    "    print(f\"Running the {i}th sample\")\n",
    "    for j in range(4):\n",
    "        print(f\"{j}th try\")\n",
    "        answer = get_model_response(math500_subset_questions[i],math500_subset_answers[i],\"math500_fail.txt\")\n",
    "        correct = checking_answer(answer,math500_subset_answers[i],\"checking_math500_fail.txt\")\n",
    "\n",
    "        if correct:\n",
    "            print(\"correct\")\n",
    "            pass\n",
    "        else:\n",
    "            passing = False\n",
    "            print(f\"Failed at the {j}th try\")\n",
    "            break\n",
    "    if passing:\n",
    "        print(f\"model passed the {i}th sample\")\n",
    "        correct_count+=1\n",
    "\n",
    "\n",
    "print(\"ACCURACY for FAIL@4: \", correct_count/len(math500_subset_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy above is 57%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude 3.5 sonnet accuracy vs fail@4 score on a subset of MATH500 is 73% and 57%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy(gpqa,gpqa_answers,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpqa\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic/claude-3.7-sonnet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy(gpqa,gpqa_answers,\"gpqa\",\"anthropic/claude-3.7-sonnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_k(gpqa,gpqa_answers,\"gpqa\",4,\"anthropic/claude-3.7-sonnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_model_response_async(\n",
    "    session: aiohttp.ClientSession,\n",
    "    prompt: str,\n",
    "    answer: str,\n",
    "    dataset: str,\n",
    "    model: str = \"openai/gpt-4\",\n",
    "    run_id: str = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Asynchronously calls the model endpoint and writes the response to a file.\n",
    "    Returns the model's extracted answer string.\n",
    "    \"\"\"\n",
    "    # Create paths based on the dataset and run info\n",
    "    base_dir = Path(\"results\") / dataset\n",
    "    run_id = run_id or f\"async_{int(time.time())}\"\n",
    "    \n",
    "    # Create output file paths\n",
    "    output_dir = base_dir / \"regular\"\n",
    "    output_file = output_dir / f\"{model.split('/')[-1]}_{run_id}.txt\"\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('OPENROUTER_API_KEY')}\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Solve the math problem provided below. \"\n",
    "                    \"At the very end of your message, provide the answer to the problem in <ANSWER> </ANSWER> tags:\\n\"\n",
    "                    \"In the answer tags, ONLY provide the answer to the problem. \"\n",
    "                    \"No other text or symbols such as $. \"\n",
    "                    \"Provide your answer in the simplest form.\\n\\n\"\n",
    "                    + prompt\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        async with session.post(url, headers=headers, json=payload) as resp:\n",
    "            if resp.status == 200:\n",
    "                data = await resp.json()\n",
    "                model_response = data['choices'][0]['message']['content']\n",
    "                # Parse out <ANSWER> tags\n",
    "                try:\n",
    "                    model_answer = model_response.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip()\n",
    "                except:\n",
    "                    model_answer = \"MISSING_ANSWER_TAGS\"\n",
    "\n",
    "                async with aiofiles.open(output_file, 'a') as f:\n",
    "                    await f.write(\"QUESTION: \" + prompt + \"\\n\")\n",
    "                    await f.write(\"MODEL RESPONSE: \" + model_response + \"\\n\\n\")\n",
    "                    await f.write(\"MODEL ANSWER: \" + model_answer + \"\\n\\n\")\n",
    "                    await f.write(\"CORRECT ANSWER: \" + answer + \"\\n\\n\")\n",
    "                    await f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "                return model_answer\n",
    "            else:\n",
    "                print(f\"Error: {resp.status}\")\n",
    "                return \"ERROR_RESPONSE\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"ERROR_RESPONSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def checking_answer_async(\n",
    "    session: aiohttp.ClientSession,\n",
    "    model_answer: str,\n",
    "    correct_answer: str,\n",
    "    dataset: str,\n",
    "    model: str = \"anthropic/claude-3-5-sonnet\",\n",
    "    run_id: str = None\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Asynchronously checks whether the model_answer and correct_answer\n",
    "    are equivalent by calling the specified model via the API endpoint.\n",
    "    Writes comparison logs to a file. Returns True or False.\n",
    "    \"\"\"\n",
    "    # Create paths based on the dataset and run info\n",
    "    base_dir = Path(\"results\") / dataset\n",
    "    run_id = run_id or f\"async_{int(time.time())}\"\n",
    "    \n",
    "    # Create output file paths\n",
    "    output_dir = base_dir / \"regular\"\n",
    "    check_file = output_dir / f\"checking_{model.split('/')[-1]}_{run_id}.txt\"\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('OPENROUTER_API_KEY')}\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Check whether or not these two answers are equivalent.\\n\"\n",
    "                    \"At the very end of your message, provide the answer to the problem in <ANSWER> </ANSWER> tags:\\n\"\n",
    "                    \"If the answers are equivalent, there should only be 'YES' in the answer tags.\\n\"\n",
    "                    \"If they aren't equivalent, there should only be 'NO' in the answer tags.\\n\"\n",
    "                    \"First answer: \" + model_answer + \"\\n\" + \"Second answer: \" + correct_answer\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        async with session.post(url, headers=headers, json=payload) as resp:\n",
    "            resp_text = await resp.text()\n",
    "            \n",
    "            if resp.status == 200:\n",
    "                data = await resp.json()\n",
    "                model_response = data['choices'][0]['message']['content']\n",
    "                # Extract <ANSWER>YES</ANSWER> or <ANSWER>NO</ANSWER>\n",
    "                try:\n",
    "                    responses_equivalent = model_response.split(\"<ANSWER>\")[1].split(\"</ANSWER>\")[0].strip()\n",
    "                except:\n",
    "                    responses_equivalent = \"NO\"\n",
    "\n",
    "                final_answer = (responses_equivalent == \"YES\")\n",
    "                \n",
    "                async with aiofiles.open(check_file, 'a') as f:\n",
    "                    await f.write(\"CORRECT ANSWER: \" + correct_answer + '\\n')\n",
    "                    await f.write(\"MODEL OUTPUT: \" + model_answer + '\\n')\n",
    "                    await f.write(\"COMPARISON ANSWER: \")\n",
    "                    await f.write(\"TRUE\\n\" if final_answer else \"FALSE\\n\")\n",
    "                    await f.write(model_response + '\\n')\n",
    "                    await f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "                return final_answer\n",
    "            else:\n",
    "                print(f\"Error: {resp.status}\")\n",
    "                print(f\"Error details: {resp_text}\")\n",
    "                return False\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in checking_answer: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_question_async(\n",
    "    session: aiohttp.ClientSession,\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    dataset: str,\n",
    "    model: str = \"openai/gpt-4\",\n",
    "    run_id: str = None,\n",
    "    delay: int = 0\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Orchestrates getting the model's response for one question and checking correctness.\n",
    "    Returns True if correct, False otherwise.\n",
    "    \"\"\"\n",
    "    if delay > 0:\n",
    "        await asyncio.sleep(delay)\n",
    "        \n",
    "    model_answer = await get_model_response_async(\n",
    "        session, question, str(answer), dataset, model, run_id\n",
    "    )\n",
    "    \n",
    "    if model_answer == \"ERROR_RESPONSE\":\n",
    "        return False  # If we got an error, treat it as incorrect.\n",
    "        \n",
    "    return await checking_answer_async(\n",
    "        session, model_answer, str(answer), dataset, model=model, run_id=run_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions = gsm8k[:5]\n",
    "gsm8k_answers = gsm8k_answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ACCURACY TESTS:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "handle_question_async() got multiple values for argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m tasks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(gsm8k_questions)):\n\u001b[1;32m     14\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     15\u001b[0m         asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\n\u001b[0;32m---> 16\u001b[0m             handle_question_async(\n\u001b[1;32m     17\u001b[0m                 session,\n\u001b[1;32m     18\u001b[0m                 gsm8k_questions[i],\n\u001b[1;32m     19\u001b[0m                 gsm8k_answers[i],\n\u001b[1;32m     20\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgsm8k_TESTASYNC.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchecking_gsm8k_TESTASYNC.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m                 model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic/claude-3.5-sonnet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m                 delay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     24\u001b[0m             )\n\u001b[1;32m     25\u001b[0m         )\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Run all question checks concurrently\u001b[39;00m\n\u001b[1;32m     29\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "\u001b[0;31mTypeError\u001b[0m: handle_question_async() got multiple values for argument 'model'"
     ]
    }
   ],
   "source": [
    "async with aiofiles.open('gsm8k_TESTASYNC.txt', 'w') as f:\n",
    "    await f.write('')\n",
    "\n",
    "# Clear out old checking_gsm8k.txt so we have a fresh file\n",
    "async with aiofiles.open('checking_gsm8k_TESTASYNC.txt', 'w') as f:\n",
    "    await f.write('')\n",
    "\n",
    "print(\"RUNNING ACCURACY TESTS:\")\n",
    "\n",
    "# We'll create the aiohttp session once and reuse it.\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    tasks = []\n",
    "    for i in range(len(gsm8k_questions)):\n",
    "        tasks.append(\n",
    "            asyncio.create_task(\n",
    "                handle_question_async(\n",
    "                    session,\n",
    "                    gsm8k_questions[i],\n",
    "                    gsm8k_answers[i],\n",
    "                    'gsm8k_TESTASYNC.txt',\n",
    "                    'checking_gsm8k_TESTASYNC.txt',\n",
    "                    model=\"anthropic/claude-3.5-sonnet\",\n",
    "                    delay=1\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Run all question checks concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    # Count how many came back True\n",
    "    total_correct = sum(results)\n",
    "    print(\"ACCURACY for gsm8k:\", total_correct / len(gsm8k_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "math500_questions = math500[:20]\n",
    "math500_answers = math500_answers[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_accuracy_test_async(\n",
    "    questions: list[str], \n",
    "    answers: list[str], \n",
    "    dataset: str, \n",
    "    model: str,\n",
    "    max_samples: int = None,\n",
    "    return_details: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Asynchronously runs an accuracy test on a dataset with a specific model.\n",
    "    \n",
    "    Args:\n",
    "        questions: List of questions to evaluate\n",
    "        answers: List of correct answers\n",
    "        dataset: Name of the dataset (gsm8k, math500, etc.)\n",
    "        model: Model identifier (e.g., \"anthropic/claude-3-5-sonnet\")\n",
    "        max_samples: Maximum number of samples to evaluate (useful for testing)\n",
    "        return_details: Whether to return detailed results or just the accuracy score\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the test results\n",
    "    \"\"\"\n",
    "    # Create a unique run ID\n",
    "    run_id = f\"async_{int(time.time())}\"\n",
    "    \n",
    "    # Limit the number of samples if specified\n",
    "    if max_samples is not None:\n",
    "        questions = questions[:max_samples]\n",
    "        answers = answers[:max_samples]\n",
    "    \n",
    "    print(f\"RUNNING ASYNC ACCURACY TEST: {model} on {dataset}\")\n",
    "    print(f\"Number of questions: {len(questions)}\")\n",
    "    \n",
    "    # Create directories\n",
    "    base_dir = Path(\"results\") / dataset\n",
    "    output_dir = base_dir / \"regular\"\n",
    "    metadata_dir = base_dir / \"metadata\"\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    metadata_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # We'll create the aiohttp session once and reuse it\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Create task list\n",
    "        tasks = []\n",
    "        for i in range(len(questions)):\n",
    "            tasks.append(\n",
    "                asyncio.create_task(\n",
    "                    handle_question_async(\n",
    "                        session,\n",
    "                        questions[i],\n",
    "                        answers[i],\n",
    "                        dataset,\n",
    "                        model=model,\n",
    "                        run_id=run_id,\n",
    "                        delay=i % 3  # Stagger requests slightly\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Run all question checks concurrently\n",
    "        print(\"Waiting for all tasks to complete...\")\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Count how many came back True\n",
    "        total_correct = sum(results)\n",
    "        accuracy = total_correct / len(questions)\n",
    "        \n",
    "        # Build result summary\n",
    "        wrong_indices = [i for i, correct in enumerate(results) if not correct]\n",
    "        summary = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"run_id\": run_id,\n",
    "            \"model\": model,\n",
    "            \"dataset\": dataset,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"num_questions\": len(questions),\n",
    "            \"num_correct\": total_correct,\n",
    "            \"wrong_indices\": wrong_indices\n",
    "        }\n",
    "        \n",
    "        # Add detailed results if requested\n",
    "        if return_details:\n",
    "            detailed_results = []\n",
    "            for i, (question, answer, is_correct) in enumerate(zip(questions, answers, results)):\n",
    "                detailed_results.append({\n",
    "                    \"index\": i,\n",
    "                    \"question\": question,\n",
    "                    \"correct_answer\": str(answer),\n",
    "                    \"is_correct\": is_correct\n",
    "                })\n",
    "            summary[\"results\"] = detailed_results\n",
    "        \n",
    "        # Save the summary as JSON\n",
    "        async with aiofiles.open(metadata_dir / f\"accuracy_async_{model.split('/')[-1]}_{run_id}.json\", 'w') as f:\n",
    "            await f.write(json.dumps(summary, indent=2))\n",
    "        \n",
    "        print(f\"ACCURACY for {model} on {dataset}: {accuracy:.4f}\")\n",
    "        \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Running an async accuracy test\n",
    "async def run_gsm8k_test():\n",
    "    result = await run_accuracy_test_async(\n",
    "        gsm8k,  # Questions\n",
    "        gsm8k_answers,  # Answers\n",
    "        \"gsm8k\",  # Dataset name\n",
    "        \"anthropic/claude-3-5-sonnet\",  # Model to use\n",
    "        max_samples=5  # Limit to 5 samples for a quick test\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# To run this: await run_gsm8k_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_fail_at_k_test_async_example():\n",
    "    result = await run_fail_at_k_test_async(\n",
    "        gsm8k,  # Questions\n",
    "        gsm8k_answers,  # Answers\n",
    "        \"gsm8k\",  # Dataset name\n",
    "        \"anthropic/claude-3-5-sonnet\",  # Model to use\n",
    "        k=2,  # Number of attempts per question\n",
    "        max_samples=3  # Limit to 3 samples for a quick test\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# To run this: await run_fail_at_k_test_async_example()\n",
    "\n",
    "# Function to run a comprehensive experiment across multiple models and k values\n",
    "def run_full_experiment(dataset=\"math500\", models=None, k_values=None, max_samples=20):\n",
    "    \"\"\"\n",
    "    Run a comprehensive experiment on a dataset with multiple models and k values.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Name of the dataset to test on\n",
    "        models: List of model identifiers to test\n",
    "        k_values: List of k values to test for fail@k\n",
    "        max_samples: Maximum number of samples to evaluate\n",
    "    \"\"\"\n",
    "    # Default values\n",
    "    models = models or [\"anthropic/claude-3-5-sonnet\", \"openai/gpt-4o\", \"google/gemini-1.5-pro\"]\n",
    "    k_values = k_values or [1, 2, 4]\n",
    "    \n",
    "    # Load the dataset\n",
    "    if dataset == \"gsm8k\":\n",
    "        questions = gsm8k[:max_samples]\n",
    "        answers = gsm8k_answers[:max_samples]\n",
    "    elif dataset == \"math500\":\n",
    "        questions = math500[:max_samples]\n",
    "        answers = math500_answers[:max_samples]\n",
    "    elif dataset == \"gpqa\":\n",
    "        questions = gpqa[:max_samples]\n",
    "        answers = gpqa_answers[:max_samples]\n",
    "    else:\n",
    "        print(f\"Unknown dataset: {dataset}\")\n",
    "        return\n",
    "    \n",
    "    # Run regular accuracy tests for each model\n",
    "    print(f\"Running accuracy tests on {dataset} with {len(questions)} questions\")\n",
    "    for model in models:\n",
    "        print(f\"\\nTesting {model}...\")\n",
    "        run_accuracy_test(\n",
    "            questions, \n",
    "            answers, \n",
    "            dataset, \n",
    "            model,\n",
    "            max_samples=max_samples\n",
    "        )\n",
    "    \n",
    "    # Run fail@k tests for each model and k value\n",
    "    print(f\"\\nRunning fail@k tests on {dataset}\")\n",
    "    for model in models:\n",
    "        for k in k_values:\n",
    "            print(f\"\\nTesting {model} with k={k}...\")\n",
    "            run_fail_at_k_test(\n",
    "                questions, \n",
    "                answers, \n",
    "                dataset, \n",
    "                model,\n",
    "                k=k,\n",
    "                max_samples=max_samples\n",
    "            )\n",
    "    \n",
    "    # Visualize the results\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    visualize_results(dataset=dataset)\n",
    "    \n",
    "    print(f\"\\nExperiment completed for {dataset}\")\n",
    "\n",
    "# Example usage:\n",
    "# run_full_experiment(dataset=\"gsm8k\", max_samples=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
